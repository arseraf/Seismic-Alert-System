{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Function for extraction and transformation of the data from the APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries imported\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import pandas_read_xml as pdx\n",
    "from functools import reduce\n",
    "from google.cloud import storage\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "#-----------------------------------Japan----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def japan_api_to_gcs():\n",
    "\n",
    "    def deep_get(dictionary, keys, default=None): # Function to get values from nested xml file\n",
    "        return reduce(lambda d, key: d.get(key, default) if isinstance(d, dict) else default, keys.split(\".\"), dictionary)\n",
    "\n",
    "    # Calling the Japan API    \n",
    "    \n",
    "    current_date = str(dt.datetime.now().date() - timedelta(days=1)) + 'T00:00:00'\n",
    "\n",
    "    df = pdx.read_xml(f'http://service.iris.edu/fdsnws/event/1/query?starttime={current_date}&orderby=time&format=xml&maxlat=45.540717058168504&minlon=129.20644141355123&maxlon=146.68542509079882&minlat=30.180889170292048')    \n",
    "    \n",
    "    json_obj = df.to_json()\n",
    "\n",
    "    json_format = json.loads(json_obj)\n",
    "\n",
    "    data = pd.json_normalize(json_format)\n",
    "\n",
    "\n",
    "    if 'q:quakeml.0.eventParameters.event' in data.keys():\n",
    "\n",
    "\n",
    "        df_events = data['q:quakeml.0.eventParameters.event']\n",
    "\n",
    "        events = []\n",
    "        for entry in df_events:\n",
    "            for l in entry:\n",
    "                events.append(l)\n",
    "\n",
    "        events_values = {'type' : [], \n",
    "                        'place' : [], \n",
    "                        'time' : [],\n",
    "                        'author' : [],\n",
    "                        'latitude' : [],\n",
    "                        'longitude' : [],\n",
    "                        'depth' : [],\n",
    "                        'mag' : [],\n",
    "                        'magType' : []}\n",
    "                        \n",
    "        for event in events:\n",
    "            events_values['type'].append(deep_get(event, 'type')) \n",
    "            events_values['place'].append(deep_get(event, 'description.text')) \n",
    "            events_values['time'].append(deep_get(event, 'origin.time.value')) \n",
    "            events_values['author'].append(deep_get(event, 'origin.creationInfo.author')) \n",
    "            events_values['latitude'].append(deep_get(event, 'origin.latitude.value')) \n",
    "            events_values['longitude'].append(deep_get(event, 'origin.longitude.value')) \n",
    "            events_values['depth'].append(deep_get(event, 'origin.depth.value')) \n",
    "            events_values['mag'].append(deep_get(event, 'magnitude.mag.value')) \n",
    "            events_values['magType'].append(deep_get(event, 'magnitude.type'))\n",
    "\n",
    "        df_api_japan = pd.DataFrame(events_values)\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_events = data[[\n",
    "        'q:quakeml.0.eventParameters.event.description.text',\n",
    "        'q:quakeml.0.eventParameters.event.origin.time.value',\n",
    "        'q:quakeml.0.eventParameters.event.origin.latitude.value',\n",
    "        'q:quakeml.0.eventParameters.event.origin.longitude.value',\n",
    "        'q:quakeml.0.eventParameters.event.origin.depth.value',\n",
    "        'q:quakeml.0.eventParameters.event.magnitude.mag.value',\n",
    "        'q:quakeml.0.eventParameters.event.magnitude.type']]\n",
    "\n",
    "        df_events.rename(columns={'q:quakeml.0.eventParameters.event.description.text' : 'place',\n",
    "        'q:quakeml.0.eventParameters.event.origin.time.value' : 'time',\n",
    "        'q:quakeml.0.eventParameters.event.origin.latitude.value' : 'latitude',\n",
    "        'q:quakeml.0.eventParameters.event.origin.longitude.value' : 'longitude',\n",
    "        'q:quakeml.0.eventParameters.event.origin.depth.value' : 'depth',\n",
    "        'q:quakeml.0.eventParameters.event.magnitude.mag.value' : 'mag',\n",
    "        'q:quakeml.0.eventParameters.event.magnitude.type' : 'magType'}, inplace=True)\n",
    "\n",
    "        df_api_japan = df_events[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']] # Data extracted\n",
    "\n",
    "    # Saving data to Cloud Storage\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_japan_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_japan.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------Transformation------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Transforming the Japan data\n",
    "\n",
    "    df_api_japan_t = df_api_japan\n",
    "    \n",
    "    df_api_japan_t['time'] = pd.to_datetime(df_api_japan_t['time'])#, unit='ns')\n",
    "\n",
    "    df_api_japan_t.latitude = df_api_japan_t.latitude.astype(float)\n",
    "    df_api_japan_t.longitude = df_api_japan_t.longitude.astype(float)\n",
    "    df_api_japan_t.depth = df_api_japan_t.depth.astype(float)\n",
    "    df_api_japan_t.mag = df_api_japan_t.mag.astype(float)\n",
    "\n",
    "    df_api_japan_t = df_api_japan_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']]\n",
    "\n",
    "    df_api_japan_t.place = df_api_japan_t.place.str.title()\n",
    "\n",
    "   \n",
    "    def correct_depth(depths): # Function to correct the depths values\n",
    "        corrected_depths = []\n",
    "        for e in depths:\n",
    "            if len(str(e)) >= 8:\n",
    "                corrected_depth = str(e)[:3]\n",
    "                corrected_depths.append(corrected_depth)\n",
    "            else:\n",
    "                corrected_depth = str(e)[:2]\n",
    "                corrected_depths.append(corrected_depth)\n",
    "        return corrected_depths\n",
    "\n",
    "    depths = df_api_japan_t.depth.to_list()\n",
    "\n",
    "    corrected_depths = correct_depth(depths)\n",
    "\n",
    "    df_api_japan_t.depth = corrected_depths\n",
    "\n",
    "    # Saving the transformed data to the Cloud Storage\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_japan_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_japan_t.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "\n",
    "#-------------------------------------------------Chile---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Calling the Chile API  \n",
    "\n",
    "def chile_api_to_cgs():\n",
    "    url = 'https://chilealerta.com/api/query/?user=demo&select=ultimos_sismos&limit=100&country=Chile'\n",
    "    json_obj = urlopen(url)\n",
    "    data = json.load(json_obj)\n",
    "    df_api_chile = pd.json_normalize(data, record_path=['ultimos_sismos_Chile'])\n",
    "\n",
    "    # Saving the data to the Cloud Storage\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_chile_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_chile.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "    #-----------------------------------Transformation----------------------------------------------------------------------\n",
    "\n",
    "    # Transforming the Chile data\n",
    "\n",
    "    df_api_chile_t = df_api_chile\n",
    "\n",
    "    df_api_chile_t.drop(columns=['state', 'local_time', 'chilean_time', 'id', 'url', 'source'], inplace=True)\n",
    "\n",
    "    df_api_chile_t.rename(columns={'utc_time' : 'time',\n",
    "                             'reference' : 'place',\n",
    "                             'magnitude' : 'mag',\n",
    "                             'scale' : 'magType'}, inplace=True)\n",
    "\n",
    "    df_api_chile_t.time = pd.to_datetime(df_api_chile_t.time)\n",
    "\n",
    "    df_api_chile_t = df_api_chile_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']] # Transformed data\n",
    "\n",
    "    # Saving the transformed data to the Cloud Storage\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_chile_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_chile_t.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "\n",
    "#--------------------------------------------------US----------------------------------------------------------------------\n",
    "\n",
    "# Calling the US API\n",
    "\n",
    "def us_api_to_cgs():\n",
    "    url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_hour.geojson'\n",
    "    json_obj = urlopen(url)\n",
    "    data = json.load(json_obj)\n",
    "\n",
    "    df_us_api = pd.json_normalize(data, record_path=['features'])\n",
    "\n",
    "    # Saving the data in the Cloud Storage\n",
    "\n",
    "    current_time = str(dt.datetime.now().strftime('%Y_%m_%d_%H_%M_%S'))\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"APIs/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_us_api.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "    #-----------------------------------Transformation--------------------------------------------------------------------\n",
    "\n",
    "    # Transforming the US data\n",
    "\n",
    "    df_api_us_t = df_us_api[['properties.time', 'properties.place', 'properties.mag', 'properties.magType', 'geometry.coordinates']]\n",
    "\n",
    "    df_api_us_t.rename(columns={'properties.time' : 'time',\n",
    "                          'properties.place' : 'place',\n",
    "                          'properties.mag' : 'mag',\n",
    "                          'properties.magType' : 'magType',\n",
    "                          'geometry.coordinates' : 'coordinates'}, inplace=True)\n",
    "\n",
    "    df_us_locations = pd.DataFrame(df_api_us_t[\"coordinates\"].to_list(), columns=['longitude', 'latitude', 'depth'])\n",
    "    df_api_us_t['latitude'] = df_us_locations.latitude\n",
    "    df_api_us_t['longitude'] = df_us_locations.longitude\n",
    "    df_api_us_t['depth'] = df_us_locations.depth\n",
    "    df_api_us_t.drop(columns='coordinates', inplace=True)\n",
    "\n",
    "    df_api_us_t.time = pd.to_datetime(df_api_us_t.time, unit='ms')\n",
    "\n",
    "    other_countries_locations = df_api_us_t[(((df_api_us_t.latitude < 20.911795455444313) & (df_api_us_t.latitude > 50.02924641916901)) & ((df_api_us_t.longitude < -124.65301770531302) & (df_api_us_t.longitude > -66.95789388605114))) | (df_api_us_t.place.str.contains('Japan|Russia|Canada|Mexico|Sea'))].index # I create the index refering to those entries\n",
    "    df_api_us_t.drop(other_countries_locations, inplace=True)\n",
    "\n",
    "    df_api_us_t = df_api_us_t[['time', 'place', 'mag', 'magType', 'depth', 'latitude', 'longitude']] # Transformed data\n",
    "\n",
    "    # Saving the transformed data to the Cloud Storage\n",
    "\n",
    "    client = storage.Client(project='Seismic Alert System')\n",
    "    bucket = client.get_bucket('seismic-data-bucket')\n",
    "    blob = bucket.blob(\"Transformed_Data/\" + f'api_us_{current_time}.csv')\n",
    "    blob.upload_from_string(df_api_us_t.to_csv(index = False),content_type = 'csv')\n",
    "\n",
    "def main(data, context): # Function called when the cloud function triggers, calling all the functions inside\n",
    "    japan_api_to_gcs()\n",
    "    chile_api_to_cgs()\n",
    "    us_api_to_cgs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2efd9f29f17dafd1e01732ce528e1fd45416f34141c32d0e667b2a58f2e9bb04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
